<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Science, Engineering, Life</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description">
<meta property="og:type" content="website">
<meta property="og:title" content="Science, Engineering, Life">
<meta property="og:url" content="http://blog.chihyang.com/index.html">
<meta property="og:site_name" content="Science, Engineering, Life">
<meta property="og:description">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Science, Engineering, Life">
<meta name="twitter:description">
  
    <link rel="alternative" href="/atom.xml" title="Science, Engineering, Life" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  <link href="http://fonts.useso.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  <link rel="stylesheet" href="/css/style.css" type="text/css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Science, Engineering, Life</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="https://www.linkedin.com/in/zhiyang">About Me</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
          <a class="main-nav-link" href="http://chihyangscience.github.io">Old Stuff</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" results="0" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="q" value="site:http://blog.chihyang.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main">
  
    <article id="post-train-general" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/23/train-general/" class="article-date">
  <time datetime="2015-05-23T09:40:06.000Z" itemprop="datePublished">2015-05-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/23/train-general/">train general Boltzmann machine</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Most general Boltzmann machine is defined as:<br>$$<br>E(v,h,\theta)=-\frac{1}{2}v^TLv-\frac{1}{2}h^TJh-v^TWh<br>$$</p>
<p>all the hidden units can be arranged into multiple hidden layers which is deep boltzmann machine. Conditional distribution given by:<br>$$<br>p(h_j=1|v,h_{-j})=\sigma(\sum^D_{i=1}W_{ij}v_i+\sum^F_{m=1-j}J_{km}h_j) \\<br>p(v_i=1|v_{-i},h)=\sigma(\sum^F_{j=1}W_{ij}h_j+\sum^D_{k=1-i}L_{ik}v_j)<br>$$</p>
<p>the learning process is given by gradient ascent of likelihood:<br>$$<br>\delta W=\alpha ([vh^T]_{data}-[vh^T]_{model}) \\<br>\delta L=\alpha ([vv^T]_{data}-[vv^T]_{model}) \\<br>\delta J=\alpha ([hh^T]_{data}-[hh^T]_{model})<br>$$</p>
<p>first item is expection with respect to data distribution $p_{data}(h,v,\theta)=p(h|v,\theta)p_{data}(v)$, where $p_{data}(v)=\frac{1}{N}\sum_n\delta(v-v_n)$, second item is expection w.r.t real model distribution, so to train the model we have to evaluate data dependent expectation and model expection.</p>
<h1 id="Model_expectation">Model expectation</h1><p>we can use Persistent Contrastive Divergence to estimate model expectation,for any exponential family $p(x,\theta)=\frac{1}{Z(\theta)}exp(\theta^T\Phi(x))$, parameter update given by: $\theta^{t+1}=\theta^{t}+\alpha_t[\Phi(x)-E_{model}[\Phi(x)]]$, the PCD approximates model expectation as follows:</p>
<ol>
<li>Randomly initialize parameter vector $\theta_0$ and M samples: $(x^{0,1},…,x^{0,M})$</li>
<li>for each iteration, keep $\theta^t$ fixed, generate another M new sample $x^{t+1,i}$ from distribution $p(x^{t+1,i}|x^{t,i},\theta^t)$</li>
<li>update $\theta^{t+1}=\theta^{t}+\alpha_t[\Phi(x)-\frac{1}{M}\sum^M_{m=1}\Phi(x^{t+1,m})]$</li>
</ol>
<p>When apply to BM model:</p>
<ol>
<li>Randomly initialize parameter vector $\theta_0$ and M samples: $(v^{0,1},h^{0,1}),…,(v^{̃0,M},h^{0,M})$</li>
<li>each iteration, running Gibbs sampler to get $(v^{t+1,i},h^{t+1,i})$ from the result of above BM conditional distribution for h and v, use these samples to update parameter for next iteration, and model expectation approximates as: $\frac{1}{M}\sum^M_{i=1}v^{t+1,i}(h^{t+1,i})^T$</li>
</ol>
<h1 id="Data_expectation">Data expectation</h1><p>To evaluate data expectation, key point is to get good approximate for $p(h|v,\theta)$, we can apply variational inference to this problem, in variational method, optimal solution happened when $Q(h,\mu)=p(h|v,\theta)$, using factoration approximation $Q(h,\mu)=\prod^F_{i=1}q(h_i), q(h_i=1)=\mu_i$, under mean field approximation:<br>$$<br>lnq(h_j)=E_{i!=j}[lnp(v,h)]+const \\<br>E_{i!=j}[lnp(v,h)]=\int lnp(v,h)\prod_{i!=j}q_idh_i<br>$$</p>
<p>apply to BM model:<br>$$<br>\mu_j=\sigma(\sum_i W_{ij}v_i+\sum_{m!=j}J_{mj}\mu_m)<br>$$</p>
<p>note here assume parameter $\theta$ is fixed, once get the $\mu_i$, we can appoximate data expectation with: $\frac{1}{N}\sum^N_{n=1}v^n(\mu^n)^T$</p>
<p>Combine two estimation process, we get following general training algorithm for BM:</p>
<ol>
<li>Randomly initialize parameter vector $\theta_0$ and M samples: $(v^{0,1},h^{0,1}),…,(v^{̃0,M},h^{0,M})$</li>
<li>for each iteration t=0 to T<ol>
<li>fix $\theta$, use variational inference get optimal $\mu_i$</li>
<li>use PCD sample m new samples by running Gibbs sampler</li>
<li>update parameters as:<br>$$<br>W^{t+1}=W^t+\alpha_t(\frac{1}{N}\sum^N_{n=1}v^n(\mu^n)^T-\frac{1}{M}\sum^M_{m=1}v^{t+1,m}(h^{t+1,m})^T) \\<br>J^{t+1}=J^t+\alpha_t(\frac{1}{N}\sum^N_{n=1}\mu^n(\mu^n)^T-\frac{1}{M}\sum^M_{m=1}h^{t+1,m}(h^{t+1,m})^T) \\<br>L^{t+1}=L^t+\alpha_t(\frac{1}{N}\sum^N_{n=1}v^n(v^n)^T-\frac{1}{M}\sum^M_{m=1}v^{t+1,m}(v^{t+1,m})^T)<br>$$</li>
</ol>
</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.chihyang.com/2015/05/23/train-general/" data-id="cia0ullqt0000lwfybr7duhdo" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Nonlinear-Dimensionality-Reduction-with-autoencoder" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/19/Nonlinear-Dimensionality-Reduction-with-autoencoder/" class="article-date">
  <time datetime="2015-05-19T00:58:08.000Z" itemprop="datePublished">2015-05-19</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/19/Nonlinear-Dimensionality-Reduction-with-autoencoder/">Nonlinear Dimensionality Reduction with autoencoder</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>We can use DBN to extract high level features, so if the dimentionality of high level feature much smaller than original feature, actually we implement nonlinear dimensionality reduction which called autoencoder.</p>
<p>To encode images, e.g. we can design a DBN with topology (28x28)-400-200-100-50-25-6, then train it with a stack of RBMs, after trained, replace activities of the binary features in each layer with deterministic real-valued probabilities. To get decoder just inverse the network, the output of decoder is just the reconstruction of original features, This is the pretrain stage.</p>
<p>In the fine-tuning stage, by using backpropagation of error derivatives make the reconstruction error as small as possible.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.chihyang.com/2015/05/19/Nonlinear-Dimensionality-Reduction-with-autoencoder/" data-id="cia0ullui000elwfyqjwntu2y" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-extract-high-level-features-using-DBN" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/15/extract-high-level-features-using-DBN/" class="article-date">
  <time datetime="2015-05-15T08:31:46.000Z" itemprop="datePublished">2015-05-15</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/15/extract-high-level-features-using-DBN/">extract high level features using DBN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Normally there are only a few labled training data set can be used, to utilize most of the unlabled dataset, we can use DBN to learn higher level features from raw input, it can capture high order correlation between lower level features, e.g. pixel correlation of images, generally will improve the performance.</p>
<p>As previously post mentioned, DBN is a probability model gives:<br>$$<br>p(v,h^1,…,h^L)=p(v|h^1)…p(h^{L-2}|h^{L-1})p(h^{L-1}，h^{L})<br>$$</p>
<p>training process is treat it as a strack of RBMs, this way DBN is approximated by:<br>$$<br>Q(h^1,…,h^L|v)=Q(h^1|v)Q(h^2|h^1)…Q(h^{L}|h^{L-1})<br>$$</p>
<p>so we get a directed gragh model, if assume posterior Q is factorized, we can approximately train DBN as:</p>
<ol>
<li>train the first layer RBM, get weight $w^1$.</li>
<li>fix $w^1$, get samples h1 from Q(h1|v) = P(h1|v, w1).</li>
<li>use the sample h1 train second RBM, get $w^2$.</li>
<li>fix $w^2$, use sample from Q(h2|v) as the data for training the 3rd layer of binary features.</li>
<li>proceed for the following layers.</li>
</ol>
<p>The posterior distribution of final output layer $Q(h^L|v)$ can be calculated using:<br>$$<br>Q(h^1|v)=\prod_{j}q(h^1_j|v), q(h^1_j=1|v)=\sigma(\sum_i w_{ij}^1v_i+a^1_j) \\<br>Q(h^l|v)=\prod_{j}q(h^l_j|v), q(h^l_j=1|v)=\sigma(\sum_i w_{ij}^lq(h_i^{l-1}=1|v)+a^l_j)<br>$$</p>
<p>We can see this result actually introduce a MLP network, well for each hidden layer, replace all hidden units with their real-valued probabilities which calculated by sigma unit.</p>
<p>So first use unlabled data train a DBN to get network parameter w, use w, we get a MLP network deduced by above $Q(h^L|v)$, we use the $Q(h^L|v)$ as high level feature, which can be used as input feature for other models. Note here training only use the unlabled data set, for labled data set, we can get their high level features, then apply these transformed feature to some supervised model, through the supervised training, again we refine the parameter w. One example is apply it to Gaussian process, which need to specify covariance function $K_{ij}=K(x_i,x_j)=\alpha exp(-\frac{1}{2\beta}(x_i-x_j)^T(x_i-x_j))$, the high level feature get from DBN is actually deduce a nonlinear mapping $F(x,w)=Q(h^L|x)$, substitue it into K:<br>$$<br>K_{ij}=\alpha exp(-\frac{1}{2\beta}(F(x_i,w)-F(x_j,w))^T(F(x_i,w)-F(x_j,w)))<br>$$</p>
<p>firstly initialize it with w which trained by DBN, and use maximum likelihood to refined w, the most important point here is via DBN:</p>
<ol>
<li>we can get a better initial w, instead of picking them randomly.</li>
<li>we can get high level feature recover high order correlation.</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.chihyang.com/2015/05/15/extract-high-level-features-using-DBN/" data-id="cia0ullu20005lwfy6poam5kq" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Deep-Belief-Network-and-training" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/10/Deep-Belief-Network-and-training/" class="article-date">
  <time datetime="2015-05-10T07:25:33.000Z" itemprop="datePublished">2015-05-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/10/Deep-Belief-Network-and-training/">Deep Belief Network and training</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Deep Belief Network(DBN) are probabilistic generative models that contain many layers of hidden variables, in which each layer captures high-order correlations between the activities of hidden features in the layer below. The top two layers of the DBN form an undirected bipartite graph with the lower layers forming a directed sigmoid belief network. We can use DBN to pretrain the unlabled data to get more useful features.</p>
<p>Consider a DBN with two hidden layers $h^1,h^2$, and assume the number of the 2nd layer hidden units is the same as the number of visible units, join distribution is:<br>$$<br>p(v,h^1,h^2,\theta)=p(v|h^1,w^1)p(h^1,h^2,w^2) \\<br>p(v|h^1,w^1)=\prod_i p(v_i|h^1,w^1) \\<br>p(v_i=1|h^1,w^1)=\sigma(\sum_jw_{ij}^1h_j^1) \\<br>p(h^1,h^2,w^2)=\frac{1}{Z}exp(h^{1T}w^2h^2)<br>$$</p>
<p>if we initialize $w^2=w^{1T}$, then:<br>$$<br>p(v,h^1,\theta)=p(v|h^1,w^1)\sum_{h2}p(h^1,h^2,w^2)\\<br>=\frac{1}{Z(W^1)}exp(\sum_{ij}w^1_{ij}v_ih_j^1) \text{ which is just the distribution of RBM consist of first two layers }<br>$$</p>
<p>the result means the two-hidden-layer DBN is at least as good as our original RBM, so we can use $w^1$ of RBM as the $w^1$ of DBN, next to get $w^2$, repeat the same process, use the sample from first RBM as input of next RBM, the trained weight as $w^2$, so recursive greedy learning procedure for the DBN is:</p>
<ol>
<li>train the first layer RBM, get weight $w^1$.</li>
<li>fix $w^1$, get samples h1 from Q(h1|v) = P(h1|v, w1).</li>
<li>use the sample h1 train second RBM, get $w^2$.</li>
<li>fix $w^2$, use sample from Q(h2|h1) = p(h2|h1,w2) as the data for training the 3rd layer of binary features.</li>
<li>proceed for the following layers.</li>
</ol>
<p>another training algorithm is approximate distribution Q has factorized form:<br>$$<br>Q(h^1,…,h^L|v)=\prod^L_{l=1}Q(h^l|v) \\<br>Q(h^1|v)=\prod_{j}q(h^1_j|v), q(h^1_j=1|v)=\sigma(\sum_i w_{ij}^1v_i+a^1_j) \\<br>Q(h^l|v)=\prod_{j}q(h^l_j|v), q(h^l_j=1|v)=\sigma(\sum_i w_{ij}^lq(h_i^{l-1}=1|v)+a^l_j)<br>$$</p>
<p>then:</p>
<ol>
<li>train the first layer RBM, get weight $w^1$.</li>
<li>fix $w^1$, get samples h1 from Q(h1|v) = P(h1|v, w1).</li>
<li>use the sample h1 train second RBM, get $w^2$.</li>
<li>fix $w^2$, use sample from Q(h2|v) as the data for training the 3rd layer of binary features.</li>
<li>proceed for the following layers.</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.chihyang.com/2015/05/10/Deep-Belief-Network-and-training/" data-id="cia0ullul000glwfyx3rd7fpu" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-RBM-for-Real-valued-data" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/10/RBM-for-Real-valued-data/" class="article-date">
  <time datetime="2015-05-10T05:00:40.000Z" itemprop="datePublished">2015-05-10</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/10/RBM-for-Real-valued-data/">RBM for Real-valued data</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>RBM can be generalized to real value input, suppose input unit $v \in R^D$ and stochastic hidden unit h is still the binary unit.</p>
<p>We treat the input v as Gaussian random variables, the energy of RBM defined as:<br>$$<br>E(v,h,\theta)=\sum^D_{i=1}\frac{(v_i-b_i)^2}{2\sigma_i^2}-\sum^D_{i=1}\sum^F_{j=1}w_{ij}h_j\frac{v_i}{\sigma_i}-\sum^F_{j=1}a_jh_j<br>$$</p>
<p>the conditional distribution then changed to:<br>$$<br>p(v_i=x|h)=\frac{1}{(2\pi\sigma_i)^{1/2}}exp(-\frac{(x-b_i-\sigma_i\sum_jh_jw_{ij})^2}{2\sigma_i^2}) \\<br>p(h_j=1|v)=\sigma(b_j+\sum_iw_{ij}\frac{v_i}{\sigma_i})<br>$$</p>
<p>to train it, the derivative of the log-likelihood with respect to w:<br>$$<br>\frac{\partial logp(v,\theta)}{\partial w_{ij}}=&lt;\frac{1}{\sigma_i}v_ih_j&gt;_{data}-&lt;\frac{1}{\sigma_i}v_ih_j&gt;_{model}<br>$$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.chihyang.com/2015/05/10/RBM-for-Real-valued-data/" data-id="cia0ullu70007lwfy2bcb1nri" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-RBM-for-Collaborative-Filtering" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/06/RBM-for-Collaborative-Filtering/" class="article-date">
  <time datetime="2015-05-06T07:31:02.000Z" itemprop="datePublished">2015-05-06</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/06/RBM-for-Collaborative-Filtering/">RBM for Collaborative Filtering</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>RBM is the best single model for collaborative filtering, suppose we have M movies, N users, and integer rating values from 1 to K. To use RBM model, use a different RBM for each user, every RBM has the same number of hidden units, but an RBM only has visible softmax units for the movies rated by that user, so an RBM has few connections if that user rated few movies, to simply the model, assume weights and biases are tied together, so if two users have rated the same movie, their two RBM’s must use the same weights between the softmax visible unit for that movie and the hidden units, an suppose a user rated m movies, let V be a K × m observed binary indicator matrix with $v_{i}^k = 1$ if the user rated movie i as k and 0 otherwise. also let $h_j$, j = 1,…,F. So this is a softmax RBM, to make prediction, need to evaluate:<br>$$<br>p(v_q^k=1|V)=\frac{p(v_q^k,V)}{p(V)} \sim \sum_h exp(-E(v_q^k,V,h))\\<br>\sim exp(v_q^kb_q^k)\prod^F_{j=1}(1+exp(\sum_{il}v_i^lw_{ij}^l+v_q^kw_{qj}^k+b_j))<br>$$</p>
<p>after train the model, pick the rating k with the maximum probability.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.chihyang.com/2015/05/06/RBM-for-Collaborative-Filtering/" data-id="cia0ullub0009lwfy9z692m6c" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-restricted-Boltzmann-machine-on-document-classification" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/03/restricted-Boltzmann-machine-on-document-classification/" class="article-date">
  <time datetime="2015-05-03T07:40:19.000Z" itemprop="datePublished">2015-05-03</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/03/restricted-Boltzmann-machine-on-document-classification/">restricted Boltzmann machine on document classification</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Restricted Boltzmann machine can be used for document classification as kind of latent factor model, and it is a unsupervised learning model.</p>
<p>Suppose K is the dictionary size and D is the document size, each word in document is a input of RBM v, F is number of document topic, each topic is a hidden binary unit h,so we are interested to get $p(H_i=1|V)$ which is the posterior probability of the topic,  V is a K × D observed binary matrix with $v_i^k = 1$ if visible unit i takes on kth value, it is a softmax RBM:<br>$$<br>p(V,h)=\frac{1}{Z}e^{-E(V,h)} \\<br>E(V,h)=-\sum^D_{i=1}\sum^F_{j=1}\sum^K_{k=1}w^k_{ij}h_jv^k_i-\sum^D_{i=1}\sum^K_{k=1}v^k_ib^k_i-\sum^F_{j=1}h_ja_j \\<br>$$</p>
<p>to make it scale with variable document size, change the hidden unit term, instead use energy:<br>$$<br>E(V,h)=-\sum^D_{i=1}\sum^F_{j=1}\sum^K_{k=1}w^k_{ij}h_jv^k_i-\sum^D_{i=1}\sum^K_{k=1}v^k_ib^k_i-D\sum^F_{j=1}h_ja_j<br>$$</p>
<p>because the document size is variable, so for each document define a RBM, to simplify the model, assume all of these softmax units can share the same set of weights, connecting them to binary hidden units and ignore the word order, so $w^k_{ij}$ independent of work input, $w^k_{ij}=w^k_j$, define $\bar{v}^k=\sum^D_{i=1}v^k_i$ to be the count of the k-th word in the document, energy can be written:<br>$$<br>E(V,h)=-\sum^F_{j=1}\sum^K_{k=1}w^k_{j}h_j\bar{v}^k-\sum^K_{k=1}\bar{v}^kb^k_i-D\sum^F_{j=1}h_ja_j<br>$$</p>
<p>conditional distribution changed to:<br>$$<br>p(h_j=1|V)=\sigma(Da_j+\sum^K_{k=1}w^k_j\bar{v}^k)<br>$$</p>
<p>The training process is same as before which use Contrastive Divergence:<br>$$<br>\delta w^k_j=\alpha ([\bar{v}^k h_j]_{data}−[\bar{v}^kh_j]_T)<br>$$</p>
<p>After train the model, for each document V, we can evaluate $p(h_j|V)$, classify it to topic j which having latest posterior probability.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.chihyang.com/2015/05/03/restricted-Boltzmann-machine-on-document-classification/" data-id="cia0ullt70003lwfyi5g8dc8m" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-ALS-collaborative-filtering-for-implicit-feedback-data" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/02/ALS-collaborative-filtering-for-implicit-feedback-data/" class="article-date">
  <time datetime="2015-05-02T07:22:56.000Z" itemprop="datePublished">2015-05-02</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/02/ALS-collaborative-filtering-for-implicit-feedback-data/">ALS collaborative filtering for implicit feedback data</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Previous post talk about ALS collaborative filtering for explict feedback data, the rating gives user’s preference on items, but in many situations, the rating is not available directly, instead we can get some implict feedback data, e.g. $r_{ui}$ express how frequently a user is buying a certain item, how much time the user view a web page. For these kind of data, there is a variant ALS collaborative filtering solution for it.</p>
<p>Normally the numerical values of implicit feedback describe the frequency of actions, it express confidence how user like or dislike the item. Introduce binary variables $p_{ui}$, indicates the preference of user u to item i, defined as:<br>$$<br>p_{ui}=1 \text{ if } r_{ui}&gt;0, \text{ else } =0<br>$$</p>
<p>to express the confidence of the preference, introdcue another variable:<br>$$<br>c_{ui}=1+\alpha r_{ui}<br>$$</p>
<p>$\alpha$ controlled by experiment, as observe more evidence for positive preference, confidence in $p_{ui} = 1$ increases accordingly. We still employ the latent factor model, assume there is user factor $x_u$, item factor $y_i$, their inner product determine the perference, that is:<br>$$<br>p_{ui}=x^T_uy_i<br>$$</p>
<p>here user and item factor should be common latent factor, because $p_{ui}$ is independent of scale, so it make sense. The cost function will be minimized is:<br>$$<br>\sum_{u,i}c_{ui}(p_{ui}-x^T_uy_i)^2+\lambda(\sum_u|x_u|^2+\sum_i|y_i|^2)<br>$$</p>
<p>which weighted by its confidence. suppose $y_i$ is i-th row of nxf matrix Y, $x_u$ is u-th row of mxf matrix X, and for each user introduce diagonal nxn matrix $C^u$ for $C^u_{ii}=c_{ui}$,and vector $p(u) \in R^n$ contains all the preferences by u(the $p_{ui}$ values), derivative w.r.t $x_u$, get:<br>$$<br>-\sum_ic_{ui}(p_{ui}-x^T_uy_i)y_i+\lambda x_u=0 \\<br>=&gt; x_u=(\sum_i c_{ui}y_iy_i^T+\lambda)^{-1}\sum_ic_{ui}p_{ui}y_i \\<br>=&gt; x_u=(Y^TC^uY+\lambda I)^{-1}Y^TC^up(u)<br>$$</p>
<p>same process get:<br>$$<br>y_i=(X^TC^iX+\lambda I)^{-1}X^TC^ip(i)<br>$$</p>
<p>here p(i) is all the preferences for i, $C^i$ is diagonal m×m matrix C where $C^i_{uu} = c_{ui}$, to evaluate $Y^TC^uY$, note: $Y^TC^uY=Y^TY+Y^T(C^u-I)Y$, there are only small fraction of non-zero elements, this will speed up the calculation.</p>
<p>When make recommendation, the inner product generally will not be 1 or 0, so recommend to user u the K items with the top K largest values of $p_{ui}$</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.chihyang.com/2015/05/02/ALS-collaborative-filtering-for-implicit-feedback-data/" data-id="cia0ullur000ilwfy4abfuu2p" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/machine-learning/">machine learning</a></li></ul>

    </footer>
  </div>
  
</article>


  
    <article id="post-Old-Blogs-here" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2015/05/01/Old-Blogs-here/" class="article-date">
  <time datetime="2015-05-01T10:56:14.000Z" itemprop="datePublished">2015-05-01</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/05/01/Old-Blogs-here/">Old Blogs here</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>Some old blogs here:<br><a href="http://chihyangscience.github.io" target="_blank" rel="external">http://chihyangscience.github.io</a><br><a href="http://blog.linkweb.me" target="_blank" rel="external">http://blog.linkweb.me</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://blog.chihyang.com/2015/05/01/Old-Blogs-here/" data-id="cia0ulluf000blwfyt4o4wvx6" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/life/">life</a></li></ul>

    </footer>
  </div>
  
</article>


  
  
</section>
        
          <aside id="sidebar">
  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/life/">life</a><span class="tag-list-count">1</span></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/machine-learning/">machine learning</a><span class="tag-list-count">8</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/life/" style="font-size: 10px;">life</a><a href="/tags/machine-learning/" style="font-size: 20px;">machine learning</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2015/05/">May 2015</a><span class="archive-list-count">9</span></li></ul>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recents</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2015/05/23/train-general/">train general Boltzmann machine</a>
          </li>
        
          <li>
            <a href="/2015/05/19/Nonlinear-Dimensionality-Reduction-with-autoencoder/">Nonlinear Dimensionality Reduction with autoencoder</a>
          </li>
        
          <li>
            <a href="/2015/05/15/extract-high-level-features-using-DBN/">extract high level features using DBN</a>
          </li>
        
          <li>
            <a href="/2015/05/10/Deep-Belief-Network-and-training/">Deep Belief Network and training</a>
          </li>
        
          <li>
            <a href="/2015/05/10/RBM-for-Real-valued-data/">RBM for Real-valued data</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2015 chih.yang@outlook.com<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="https://www.linkedin.com/in/zhiyang" class="mobile-nav-link">About Me</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="http://chihyangscience.github.io" class="mobile-nav-link">Old Stuff</a>
  
</nav>
    

<script src="//ajax.useso.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css" type="text/css">
  <script src="/fancybox/jquery.fancybox.pack.js" type="text/javascript"></script>


<script src="/js/script.js" type="text/javascript"></script>

<script type="text/x-mathjax-config"> 
MathJax.Hub.Config({ 
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} 
}); 
</script>
<script type="text/javascript"
   src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  </div>
</body>
</html>