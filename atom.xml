<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Science, Engineering, Life]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://blog.chihyang.com/"/>
  <updated>2015-05-10T11:45:35.000Z</updated>
  <id>http://blog.chihyang.com/</id>
  
  <author>
    <name><![CDATA[chih.yang@outlook.com]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Deep Belief Network and training]]></title>
    <link href="http://blog.chihyang.com/2015/05/10/Deep-Belief-Network-and-training/"/>
    <id>http://blog.chihyang.com/2015/05/10/Deep-Belief-Network-and-training/</id>
    <published>2015-05-10T07:25:33.000Z</published>
    <updated>2015-05-10T11:45:35.000Z</updated>
    <content type="html"><![CDATA[<p>Deep Belief Network(DBN) are probabilistic generative models that contain many layers of hidden variables, in which each layer captures high-order correlations between the activities of hidden features in the layer below. The top two layers of the DBN form an undirected bipartite graph with the lower layers forming a directed sigmoid belief network. We can use DBN to pretrain the unlabled data to get more useful features.</p>
<p>Consider a DBN with two hidden layers $h^1,h^2$, and assume the number of the 2nd layer hidden units is the same as the number of visible units, join distribution is:<br>$$<br>p(v,h^1,h^2,\theta)=p(v|h^1,w^1)p(h^1,h^2,w^2) \\<br>p(v|h^1,w^1)=\prod_i p(v_i|h^1,w^1) \\<br>p(v_i=1|h^1,w^1)=\sigma(\sum_jw_{ij}^1h_j^1) \\<br>p(h^1,h^2,w^2)=\frac{1}{Z}exp(h^{1T}w^2h^2)<br>$$</p>
<p>if we initialize $w^2=w^{1T}$, then:<br>$$<br>p(v,h^1,\theta)=p(v|h^1,w^1)\sum_{h2}p(h^1,h^2,w^2)\\<br>=\frac{1}{Z(W^1)}exp(\sum_{ij}w^1_{ij}v_ih_j^1) \text{ which is just the distribution of RBM consist of first two layers }<br>$$</p>
<p>the result means the two-hidden-layer DBN is at least as good as our original RBM, so we can use $w^1$ of RBM as the $w^1$ of DBN, next to get $w^2$, repeat the same process, use the sample from first RBM as input of next RBM, the trained weight as $w^2$, so recursive greedy learning procedure for the DBN is:</p>
<ol>
<li>train the first layer RBM, get weight $w^1$.</li>
<li>fix $w^1$, get samples h1 from Q(h1|v) = P(h1|v, w1).</li>
<li>use the sample h1 train second RBM, get $w^2$.</li>
<li>fix $w^2$, use sample from Q(h2|h1) = p(h2|h1,w2) as the data for training the 3rd layer of binary features.</li>
<li>proceed for the following layers.</li>
</ol>
<p>another training algorithm is approximate distribution Q has factorized form:<br>$$<br>Q(h^1,…,h^L|v)=\prod^L_{l=1}Q(h^l|v) \\<br>Q(h^1|v)=\prod_{j}q(h^1_j|v), q(h^1_j=1|v)=\sigma(\sum_i w_{ij}^1v_i+a^1_j) \\<br>Q(h^l|v)=\prod_{j}q(h^l_j|v), q(h^l_j=1|v)=\sigma(\sum_i w_{ij}^lq(h_i^{l-1}=1|v)+a^l_j)<br>$$</p>
<p>then:</p>
<ol>
<li>train the first layer RBM, get weight $w^1$.</li>
<li>fix $w^1$, get samples h1 from Q(h1|v) = P(h1|v, w1).</li>
<li>use the sample h1 train second RBM, get $w^2$.</li>
<li>fix $w^2$, use sample from Q(h2|v) as the data for training the 3rd layer of binary features.</li>
<li>proceed for the following layers.</li>
</ol>
]]></content>
    <summary type="html">
    <![CDATA[<p>Deep Belief Network(DBN) are probabilistic generative models that contain many layers of hidden variables, in which each layer captures h]]>
    </summary>
    
      <category term="machine learning" scheme="http://blog.chihyang.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[RBM for Real-valued data]]></title>
    <link href="http://blog.chihyang.com/2015/05/10/RBM-for-Real-valued-data/"/>
    <id>http://blog.chihyang.com/2015/05/10/RBM-for-Real-valued-data/</id>
    <published>2015-05-10T05:00:40.000Z</published>
    <updated>2015-05-10T05:23:13.000Z</updated>
    <content type="html"><![CDATA[<p>RBM can be generalized to real value input, suppose input unit $v \in R^D$ and stochastic hidden unit h is still the binary unit.</p>
<p>We treat the input v as Gaussian random variables, the energy of RBM defined as:<br>$$<br>E(v,h,\theta)=\sum^D_{i=1}\frac{(v_i-b_i)^2}{2\sigma_i^2}-\sum^D_{i=1}\sum^F_{j=1}w_{ij}h_j\frac{v_i}{\sigma_i}-\sum^F_{j=1}a_jh_j<br>$$</p>
<p>the conditional distribution then changed to:<br>$$<br>p(v_i=x|h)=\frac{1}{(2\pi\sigma_i)^{1/2}}exp(-\frac{(x-b_i-\sigma_i\sum_jh_jw_{ij})^2}{2\sigma_i^2}) \\<br>p(h_j=1|v)=\sigma(b_j+\sum_iw_{ij}\frac{v_i}{\sigma_i})<br>$$</p>
<p>to train it, the derivative of the log-likelihood with respect to w:<br>$$<br>\frac{\partial logp(v,\theta)}{\partial w_{ij}}=&lt;\frac{1}{\sigma_i}v_ih_j&gt;_{data}-&lt;\frac{1}{\sigma_i}v_ih_j&gt;_{model}<br>$$</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>RBM can be generalized to real value input, suppose input unit $v \in R^D$ and stochastic hidden unit h is still the binary unit.</p>
<p>]]>
    </summary>
    
      <category term="machine learning" scheme="http://blog.chihyang.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[RBM for Collaborative Filtering]]></title>
    <link href="http://blog.chihyang.com/2015/05/06/RBM-for-Collaborative-Filtering/"/>
    <id>http://blog.chihyang.com/2015/05/06/RBM-for-Collaborative-Filtering/</id>
    <published>2015-05-06T07:31:02.000Z</published>
    <updated>2015-05-06T08:27:53.000Z</updated>
    <content type="html"><![CDATA[<p>RBM is the best single model for collaborative filtering, suppose we have M movies, N users, and integer rating values from 1 to K. To use RBM model, use a different RBM for each user, every RBM has the same number of hidden units, but an RBM only has visible softmax units for the movies rated by that user, so an RBM has few connections if that user rated few movies, to simply the model, assume weights and biases are tied together, so if two users have rated the same movie, their two RBM’s must use the same weights between the softmax visible unit for that movie and the hidden units, an suppose a user rated m movies, let V be a K × m observed binary indicator matrix with $v_{i}^k = 1$ if the user rated movie i as k and 0 otherwise. also let $h_j$, j = 1,…,F. So this is a softmax RBM, to make prediction, need to evaluate:<br>$$<br>p(v_q^k=1|V)=\frac{p(v_q^k,V)}{p(V)} \sim \sum_h exp(-E(v_q^k,V,h))\\<br>\sim exp(v_q^kb_q^k)\prod^F_{j=1}(1+exp(\sum_{il}v_i^lw_{ij}^l+v_q^kw_{qj}^k+b_j))<br>$$</p>
<p>after train the model, pick the rating k with the maximum probability.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>RBM is the best single model for collaborative filtering, suppose we have M movies, N users, and integer rating values from 1 to K. To us]]>
    </summary>
    
      <category term="machine learning" scheme="http://blog.chihyang.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[restricted Boltzmann machine on document classification]]></title>
    <link href="http://blog.chihyang.com/2015/05/03/restricted-Boltzmann-machine-on-document-classification/"/>
    <id>http://blog.chihyang.com/2015/05/03/restricted-Boltzmann-machine-on-document-classification/</id>
    <published>2015-05-03T07:40:19.000Z</published>
    <updated>2015-05-03T09:28:54.000Z</updated>
    <content type="html"><![CDATA[<p>Restricted Boltzmann machine can be used for document classification as kind of latent factor model, and it is a unsupervised learning model.</p>
<p>Suppose K is the dictionary size and D is the document size, each word in document is a input of RBM v, F is number of document topic, each topic is a hidden binary unit h,so we are interested to get $p(H_i=1|V)$ which is the posterior probability of the topic,  V is a K × D observed binary matrix with $v_i^k = 1$ if visible unit i takes on kth value, it is a softmax RBM:<br>$$<br>p(V,h)=\frac{1}{Z}e^{-E(V,h)} \\<br>E(V,h)=-\sum^D_{i=1}\sum^F_{j=1}\sum^K_{k=1}w^k_{ij}h_jv^k_i-\sum^D_{i=1}\sum^K_{k=1}v^k_ib^k_i-\sum^F_{j=1}h_ja_j \\<br>$$</p>
<p>to make it scale with variable document size, change the hidden unit term, instead use energy:<br>$$<br>E(V,h)=-\sum^D_{i=1}\sum^F_{j=1}\sum^K_{k=1}w^k_{ij}h_jv^k_i-\sum^D_{i=1}\sum^K_{k=1}v^k_ib^k_i-D\sum^F_{j=1}h_ja_j<br>$$</p>
<p>because the document size is variable, so for each document define a RBM, to simplify the model, assume all of these softmax units can share the same set of weights, connecting them to binary hidden units and ignore the word order, so $w^k_{ij}$ independent of work input, $w^k_{ij}=w^k_j$, define $\bar{v}^k=\sum^D_{i=1}v^k_i$ to be the count of the k-th word in the document, energy can be written:<br>$$<br>E(V,h)=-\sum^F_{j=1}\sum^K_{k=1}w^k_{j}h_j\bar{v}^k-\sum^K_{k=1}\bar{v}^kb^k_i-D\sum^F_{j=1}h_ja_j<br>$$</p>
<p>conditional distribution changed to:<br>$$<br>p(h_j=1|V)=\sigma(Da_j+\sum^K_{k=1}w^k_j\bar{v}^k)<br>$$</p>
<p>The training process is same as before which use Contrastive Divergence:<br>$$<br>\delta w^k_j=\alpha ([\bar{v}^k h_j]_{data}−[\bar{v}^kh_j]_T)<br>$$</p>
<p>After train the model, for each document V, we can evaluate $p(h_j|V)$, classify it to topic j which having latest posterior probability.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Restricted Boltzmann machine can be used for document classification as kind of latent factor model, and it is a unsupervised learning mo]]>
    </summary>
    
      <category term="machine learning" scheme="http://blog.chihyang.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ALS collaborative filtering for implicit feedback data]]></title>
    <link href="http://blog.chihyang.com/2015/05/02/ALS-collaborative-filtering-for-implicit-feedback-data/"/>
    <id>http://blog.chihyang.com/2015/05/02/ALS-collaborative-filtering-for-implicit-feedback-data/</id>
    <published>2015-05-02T07:22:56.000Z</published>
    <updated>2015-05-02T09:36:18.000Z</updated>
    <content type="html"><![CDATA[<p>Previous post talk about ALS collaborative filtering for explict feedback data, the rating gives user’s preference on items, but in many situations, the rating is not available directly, instead we can get some implict feedback data, e.g. $r_{ui}$ express how frequently a user is buying a certain item, how much time the user view a web page. For these kind of data, there is a variant ALS collaborative filtering solution for it.</p>
<p>Normally the numerical values of implicit feedback describe the frequency of actions, it express confidence how user like or dislike the item. Introduce binary variables $p_{ui}$, indicates the preference of user u to item i, defined as:<br>$$<br>p_{ui}=1 \text{ if } r_{ui}&gt;0, \text{ else } =0<br>$$</p>
<p>to express the confidence of the preference, introdcue another variable:<br>$$<br>c_{ui}=1+\alpha r_{ui}<br>$$</p>
<p>$\alpha$ controlled by experiment, as observe more evidence for positive preference, confidence in $p_{ui} = 1$ increases accordingly. We still employ the latent factor model, assume there is user factor $x_u$, item factor $y_i$, their inner product determine the perference, that is:<br>$$<br>p_{ui}=x^T_uy_i<br>$$</p>
<p>here user and item factor should be common latent factor, because $p_{ui}$ is independent of scale, so it make sense. The cost function will be minimized is:<br>$$<br>\sum_{u,i}c_{ui}(p_{ui}-x^T_uy_i)^2+\lambda(\sum_u|x_u|^2+\sum_i|y_i|^2)<br>$$</p>
<p>which weighted by its confidence. suppose $y_i$ is i-th row of nxf matrix Y, $x_u$ is u-th row of mxf matrix X, and for each user introduce diagonal nxn matrix $C^u$ for $C^u_{ii}=c_{ui}$,and vector $p(u) \in R^n$ contains all the preferences by u(the $p_{ui}$ values), derivative w.r.t $x_u$, get:<br>$$<br>-\sum_ic_{ui}(p_{ui}-x^T_uy_i)y_i+\lambda x_u=0 \\<br>=&gt; x_u=(\sum_i c_{ui}y_iy_i^T+\lambda)^{-1}\sum_ic_{ui}p_{ui}y_i \\<br>=&gt; x_u=(Y^TC^uY+\lambda I)^{-1}Y^TC^up(u)<br>$$</p>
<p>same process get:<br>$$<br>y_i=(X^TC^iX+\lambda I)^{-1}X^TC^ip(i)<br>$$</p>
<p>here p(i) is all the preferences for i, $C^i$ is diagonal m×m matrix C where $C^i_{uu} = c_{ui}$, to evaluate $Y^TC^uY$, note: $Y^TC^uY=Y^TY+Y^T(C^u-I)Y$, there are only small fraction of non-zero elements, this will speed up the calculation.</p>
<p>When make recommendation, the inner product generally will not be 1 or 0, so recommend to user u the K items with the top K largest values of $p_{ui}$</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Previous post talk about ALS collaborative filtering for explict feedback data, the rating gives user’s preference on items, but in many ]]>
    </summary>
    
      <category term="machine learning" scheme="http://blog.chihyang.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Old Blogs here]]></title>
    <link href="http://blog.chihyang.com/2015/05/01/Old-Blogs-here/"/>
    <id>http://blog.chihyang.com/2015/05/01/Old-Blogs-here/</id>
    <published>2015-05-01T10:56:14.000Z</published>
    <updated>2015-05-01T11:02:35.000Z</updated>
    <content type="html"><![CDATA[<p>Some old blogs here:<br><a href="http://chihyangscience.github.io" target="_blank" rel="external">http://chihyangscience.github.io</a><br><a href="http://blog.linkweb.me" target="_blank" rel="external">http://blog.linkweb.me</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Some old blogs here:<br><a href="http://chihyangscience.github.io" target="_blank" rel="external">http://chihyangscience.github.io</a><br]]>
    </summary>
    
      <category term="life" scheme="http://blog.chihyang.com/tags/life/"/>
    
  </entry>
  
</feed>