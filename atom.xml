<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title><![CDATA[Science, Engineering, Life]]></title>
  
  <link href="/atom.xml" rel="self"/>
  <link href="http://blog.chihyang.com/"/>
  <updated>2015-05-10T05:23:13.000Z</updated>
  <id>http://blog.chihyang.com/</id>
  
  <author>
    <name><![CDATA[chih.yang@outlook.com]]></name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[RBM for Real-valued data]]></title>
    <link href="http://blog.chihyang.com/2015/05/10/RBM-for-Real-valued-data/"/>
    <id>http://blog.chihyang.com/2015/05/10/RBM-for-Real-valued-data/</id>
    <published>2015-05-10T05:00:40.000Z</published>
    <updated>2015-05-10T05:23:13.000Z</updated>
    <content type="html"><![CDATA[<p>RBM can be generalized to real value input, suppose input unit $v \in R^D$ and stochastic hidden unit h is still the binary unit.</p>
<p>We treat the input v as Gaussian random variables, the energy of RBM defined as:<br>$$<br>E(v,h,\theta)=\sum^D_{i=1}\frac{(v_i-b_i)^2}{2\sigma_i^2}-\sum^D_{i=1}\sum^F_{j=1}w_{ij}h_j\frac{v_i}{\sigma_i}-\sum^F_{j=1}a_jh_j<br>$$</p>
<p>the conditional distribution then changed to:<br>$$<br>p(v_i=x|h)=\frac{1}{(2\pi\sigma_i)^{1/2}}exp(-\frac{(x-b_i-\sigma_i\sum_jh_jw_{ij})^2}{2\sigma_i^2}) \\<br>p(h_j=1|v)=\sigma(b_j+\sum_iw_{ij}\frac{v_i}{\sigma_i})<br>$$</p>
<p>to train it, the derivative of the log-likelihood with respect to w:<br>$$<br>\frac{\partial logp(v,\theta)}{\partial w_{ij}}=&lt;\frac{1}{\sigma_i}v_ih_j&gt;_{data}-&lt;\frac{1}{\sigma_i}v_ih_j&gt;_{model}<br>$$</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>RBM can be generalized to real value input, suppose input unit $v \in R^D$ and stochastic hidden unit h is still the binary unit.</p>
<p>]]>
    </summary>
    
      <category term="machine learning" scheme="http://blog.chihyang.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[RBM for Collaborative Filtering]]></title>
    <link href="http://blog.chihyang.com/2015/05/06/RBM-for-Collaborative-Filtering/"/>
    <id>http://blog.chihyang.com/2015/05/06/RBM-for-Collaborative-Filtering/</id>
    <published>2015-05-06T07:31:02.000Z</published>
    <updated>2015-05-06T08:27:53.000Z</updated>
    <content type="html"><![CDATA[<p>RBM is the best single model for collaborative filtering, suppose we have M movies, N users, and integer rating values from 1 to K. To use RBM model, use a different RBM for each user, every RBM has the same number of hidden units, but an RBM only has visible softmax units for the movies rated by that user, so an RBM has few connections if that user rated few movies, to simply the model, assume weights and biases are tied together, so if two users have rated the same movie, their two RBM’s must use the same weights between the softmax visible unit for that movie and the hidden units, an suppose a user rated m movies, let V be a K × m observed binary indicator matrix with $v_{i}^k = 1$ if the user rated movie i as k and 0 otherwise. also let $h_j$, j = 1,…,F. So this is a softmax RBM, to make prediction, need to evaluate:<br>$$<br>p(v_q^k=1|V)=\frac{p(v_q^k,V)}{p(V)} \sim \sum_h exp(-E(v_q^k,V,h))\\<br>\sim exp(v_q^kb_q^k)\prod^F_{j=1}(1+exp(\sum_{il}v_i^lw_{ij}^l+v_q^kw_{qj}^k+b_j))<br>$$</p>
<p>after train the model, pick the rating k with the maximum probability.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>RBM is the best single model for collaborative filtering, suppose we have M movies, N users, and integer rating values from 1 to K. To us]]>
    </summary>
    
      <category term="machine learning" scheme="http://blog.chihyang.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[restricted Boltzmann machine on document classification]]></title>
    <link href="http://blog.chihyang.com/2015/05/03/restricted-Boltzmann-machine-on-document-classification/"/>
    <id>http://blog.chihyang.com/2015/05/03/restricted-Boltzmann-machine-on-document-classification/</id>
    <published>2015-05-03T07:40:19.000Z</published>
    <updated>2015-05-03T09:28:54.000Z</updated>
    <content type="html"><![CDATA[<p>Restricted Boltzmann machine can be used for document classification as kind of latent factor model, and it is a unsupervised learning model.</p>
<p>Suppose K is the dictionary size and D is the document size, each word in document is a input of RBM v, F is number of document topic, each topic is a hidden binary unit h,so we are interested to get $p(H_i=1|V)$ which is the posterior probability of the topic,  V is a K × D observed binary matrix with $v_i^k = 1$ if visible unit i takes on kth value, it is a softmax RBM:<br>$$<br>p(V,h)=\frac{1}{Z}e^{-E(V,h)} \\<br>E(V,h)=-\sum^D_{i=1}\sum^F_{j=1}\sum^K_{k=1}w^k_{ij}h_jv^k_i-\sum^D_{i=1}\sum^K_{k=1}v^k_ib^k_i-\sum^F_{j=1}h_ja_j \\<br>$$</p>
<p>to make it scale with variable document size, change the hidden unit term, instead use energy:<br>$$<br>E(V,h)=-\sum^D_{i=1}\sum^F_{j=1}\sum^K_{k=1}w^k_{ij}h_jv^k_i-\sum^D_{i=1}\sum^K_{k=1}v^k_ib^k_i-D\sum^F_{j=1}h_ja_j<br>$$</p>
<p>because the document size is variable, so for each document define a RBM, to simplify the model, assume all of these softmax units can share the same set of weights, connecting them to binary hidden units and ignore the word order, so $w^k_{ij}$ independent of work input, $w^k_{ij}=w^k_j$, define $\bar{v}^k=\sum^D_{i=1}v^k_i$ to be the count of the k-th word in the document, energy can be written:<br>$$<br>E(V,h)=-\sum^F_{j=1}\sum^K_{k=1}w^k_{j}h_j\bar{v}^k-\sum^K_{k=1}\bar{v}^kb^k_i-D\sum^F_{j=1}h_ja_j<br>$$</p>
<p>conditional distribution changed to:<br>$$<br>p(h_j=1|V)=\sigma(Da_j+\sum^K_{k=1}w^k_j\bar{v}^k)<br>$$</p>
<p>The training process is same as before which use Contrastive Divergence:<br>$$<br>\delta w^k_j=\alpha ([\bar{v}^k h_j]_{data}−[\bar{v}^kh_j]_T)<br>$$</p>
<p>After train the model, for each document V, we can evaluate $p(h_j|V)$, classify it to topic j which having latest posterior probability.</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Restricted Boltzmann machine can be used for document classification as kind of latent factor model, and it is a unsupervised learning mo]]>
    </summary>
    
      <category term="machine learning" scheme="http://blog.chihyang.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[ALS collaborative filtering for implicit feedback data]]></title>
    <link href="http://blog.chihyang.com/2015/05/02/ALS-collaborative-filtering-for-implicit-feedback-data/"/>
    <id>http://blog.chihyang.com/2015/05/02/ALS-collaborative-filtering-for-implicit-feedback-data/</id>
    <published>2015-05-02T07:22:56.000Z</published>
    <updated>2015-05-02T09:36:18.000Z</updated>
    <content type="html"><![CDATA[<p>Previous post talk about ALS collaborative filtering for explict feedback data, the rating gives user’s preference on items, but in many situations, the rating is not available directly, instead we can get some implict feedback data, e.g. $r_{ui}$ express how frequently a user is buying a certain item, how much time the user view a web page. For these kind of data, there is a variant ALS collaborative filtering solution for it.</p>
<p>Normally the numerical values of implicit feedback describe the frequency of actions, it express confidence how user like or dislike the item. Introduce binary variables $p_{ui}$, indicates the preference of user u to item i, defined as:<br>$$<br>p_{ui}=1 \text{ if } r_{ui}&gt;0, \text{ else } =0<br>$$</p>
<p>to express the confidence of the preference, introdcue another variable:<br>$$<br>c_{ui}=1+\alpha r_{ui}<br>$$</p>
<p>$\alpha$ controlled by experiment, as observe more evidence for positive preference, confidence in $p_{ui} = 1$ increases accordingly. We still employ the latent factor model, assume there is user factor $x_u$, item factor $y_i$, their inner product determine the perference, that is:<br>$$<br>p_{ui}=x^T_uy_i<br>$$</p>
<p>here user and item factor should be common latent factor, because $p_{ui}$ is independent of scale, so it make sense. The cost function will be minimized is:<br>$$<br>\sum_{u,i}c_{ui}(p_{ui}-x^T_uy_i)^2+\lambda(\sum_u|x_u|^2+\sum_i|y_i|^2)<br>$$</p>
<p>which weighted by its confidence. suppose $y_i$ is i-th row of nxf matrix Y, $x_u$ is u-th row of mxf matrix X, and for each user introduce diagonal nxn matrix $C^u$ for $C^u_{ii}=c_{ui}$,and vector $p(u) \in R^n$ contains all the preferences by u(the $p_{ui}$ values), derivative w.r.t $x_u$, get:<br>$$<br>-\sum_ic_{ui}(p_{ui}-x^T_uy_i)y_i+\lambda x_u=0 \\<br>=&gt; x_u=(\sum_i c_{ui}y_iy_i^T+\lambda)^{-1}\sum_ic_{ui}p_{ui}y_i \\<br>=&gt; x_u=(Y^TC^uY+\lambda I)^{-1}Y^TC^up(u)<br>$$</p>
<p>same process get:<br>$$<br>y_i=(X^TC^iX+\lambda I)^{-1}X^TC^ip(i)<br>$$</p>
<p>here p(i) is all the preferences for i, $C^i$ is diagonal m×m matrix C where $C^i_{uu} = c_{ui}$, to evaluate $Y^TC^uY$, note: $Y^TC^uY=Y^TY+Y^T(C^u-I)Y$, there are only small fraction of non-zero elements, this will speed up the calculation.</p>
<p>When make recommendation, the inner product generally will not be 1 or 0, so recommend to user u the K items with the top K largest values of $p_{ui}$</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Previous post talk about ALS collaborative filtering for explict feedback data, the rating gives user’s preference on items, but in many ]]>
    </summary>
    
      <category term="machine learning" scheme="http://blog.chihyang.com/tags/machine-learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Old Blogs here]]></title>
    <link href="http://blog.chihyang.com/2015/05/01/Old-Blogs-here/"/>
    <id>http://blog.chihyang.com/2015/05/01/Old-Blogs-here/</id>
    <published>2015-05-01T10:56:14.000Z</published>
    <updated>2015-05-01T11:02:35.000Z</updated>
    <content type="html"><![CDATA[<p>Some old blogs here:<br><a href="http://chihyangscience.github.io" target="_blank" rel="external">http://chihyangscience.github.io</a><br><a href="http://blog.linkweb.me" target="_blank" rel="external">http://blog.linkweb.me</a></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>Some old blogs here:<br><a href="http://chihyangscience.github.io" target="_blank" rel="external">http://chihyangscience.github.io</a><br]]>
    </summary>
    
      <category term="life" scheme="http://blog.chihyang.com/tags/life/"/>
    
  </entry>
  
</feed>